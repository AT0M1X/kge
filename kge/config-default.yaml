job:
  type: train                 # train, eval, grid, ...
  device: 'cuda'

## INPUT/OUTPUT ################################################################

dataset:
  name: 'toy'

  # training, validation, and test data files have the following fields, all
  # tab-separated:
  # - 0: subject index
  # - 1: relation index
  # - 2: object index
  # - 3-...: arbitrary metadata fields
  # Indexes are assumed to be dense throughout.
  train: train.del
  valid: valid.del
  test: test.del

  # entity and relation maps store auxiliary information about each
  # entity/predicate. Fields are tab-separated:
  # - 0: entity/predicate index (as in train/valid/test)
  # - 1...: arbitrary metadata fields
  entity_map: entity_map.del
  relation_map: relation_map.del

output:
  # Main folder where logs, checkpoints, traces, and model output are stored.
  # If left unspecified, defaults to local/experiments/<date>-<dataset>-<model>
  folder: ''
  logfile: 'kge.log'
  tracefile: 'trace.yaml'   # one yaml entry per line

## MODEL #######################################################################
## Model and its hyperparameters. Used for all jobs.

model:
  type: complex               # distmult, complex, transe, conve
  dim: 100                    # entity dimensionality or [ entity, relation ] dimensionality
  embedder: lookup            # lookup, single or pair as above

## Parameters used for embeddings. If a single value is specified, it's used for
## both entites and relations. If an array is specified, first entry is used for
## entity embeddings, second for relation embeddings.
lookup_embedder:
  initialize: normal          # xavier, uniform, normal
  initialize_arg: 0.1         # gain for Xavier, range for uniform, stddev for Normal
  dropout: 0                  # dropout used for embeddings
  # TODO l2_reg: 0.01                # use L2 regularization
  sparse: False               # ??
  normalize: ''               # alternatively: normalize '', L2

# TODO model-specific parametrs
# SRT:

## TRAINING ####################################################################
## Used for training jobs.

train:
  type: 1toN                  # 1toN, negative_sampling
  max_epochs: 20
  optimizer: adagrad          # sgd, adagrad, adam
  batch_size: 100
  lr: 0.01                    # learning rate; TODO: schedule
  loss: bce                   # bce, TODO
  num_workers: 0
  pin_memory: False

# configuration options for model validation/selection during training
valid:
  every: 2                    # epochs (disable with 0)
  metric: mrr                 # mrr or hits@k; k specified under evaluate
  early_abort: False

checkpoint:
  every: 0                    # epochs (disable with 0)
  basefile: 'checkpoint'      # add _epoch.pt

## EVALUATION ##################################################################
## Used for evaluation jobs.

# fixed metrics: compute MRR and HITS@1, ..., HITS@k
eval:
  type: entity_ranking
  max_k: 10                       # maximum k for HITS@k
  trace_examples: True            # trace rank/filtered rank of each example
  batch_size: 100
  num_workers: 0
  pin_memory: False


## GRID SEARCH #################################################################

# array of pairs of parameter name (flattened) and possible values
grid:
  run: True                    # if false, only creates folders and configurations
  options: [
    [ 'train.lr', [ 0.001, 0.01, 0.1 ] ],
    [ 'model.dim', [ 100, 200 ] ]
  ]

## Used for evaluation jobs.

## HYPERPARAMETER TUNING #######################################################

# TODO grid-search here? -> changes other configuration paramters in some
# systmatic way (including output fold)
# grid_search:
#   train.lr: = [ 0.01, 0.001 ]
